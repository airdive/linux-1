HBase 性能优化方法总结

表的设计

Pre-Creating Regions

默认情况下 在创建 HBase 表的时候会自动创建一个 region 分区 当导入数据的时候 所有的 HBase 客户端都向这一个 region 写数据
直到这个 region 足够大了才进行切分 一种可以加快批量写入速度的方法是通过预先创建一些空的 regions 这样当数据写入 HBase 时
会按照 region 分区情况 在集群内做数据的负载均衡

public static boolean createTable(HBaseAdmin admin, HTableDescriptor table, byte[][] splits)
throws IOException {
    try {
        admin.createTable(table, splits);
        return true;
    } catch (TableExistsException e) {
        logger.info("table" + table.getNameAsString() + " already exists");
        return false;
    }
}

public static byte[][] getHexSplits (String startKey, String endKey, int numRegions) {
    byte[][] splits = new byte[numRegions - 1][];
    BigInteger lowestKey = new BigInteger(startKey, 16);
    BigInteger highestKey = new BigInteger(endKey, 16);
    BigInteger range = new BigInteger(lowestKey);
    BigInteger regionIncrement = range.divide(BigInteger.valueOf(numRegions));
    lowestKey = lowestKey.add(regionIncrement);
    for (int i = 0; i < numRegions - 1; i ++) {
        byte[] b = String.format("%016x", key).getBytes();
        splits[i] = b;
    }
    return splits;
}


Row key
HBase 中 row key 用来检索表中的记录 支持以下三种方式
通过单个 row key 访问 即按照某个 row key 键值进行 get 操作
通过 row key 的 range 进行 scan 即通过设置 startRowKey 和 endRowKey 在这个范围内进行扫描
全表扫描 即直接扫描整张表中所有行记录

在 HBase 中 row key 可以是任意字符串 最大长度 64KB 实际应用中一般为 10~100bytes 存为 byte[] 字节数组  
一般设计成定长的

row key 是按照字典序存储 因此 设计 row key 时 要充分利用这个排序特点 将经常一起读取的数据存储到一块 
将最近可能会被访问的数据放在一起

举个例子 如果最近写入 HBase 表中的数据是最可能被访问的 可以考虑将时间戳作为 row key 的一部分 由于是字典排序
所以可以使用 Long.MAX_VALUE - timestamp 作为 row key, 这样能保证新写入的数据在读取时可以被快速命中

Column Family
不要在一张表里定义太多的 column family 目前 Hbase 并不能很好的处理超过 2~3 个 column family 的表 
因为某个 column family 在 flush 的时候 它邻近的 column family 也会因关联效应被触发 flush 最终导致
系统产生更多的 I/O 感兴趣的同学可以对自己的 HBase 集群进行实际测试 从得到的测试结果数据验证一下

In Memory
创建表的时候 可以通过 HColumnDescriptor.setInMemory(true) 将表放到 RegionServer 的缓存中 
保证在读取的时候被 cache 命中

Max Version
创建表的时候 可以通过 HColumnDescriptor.setMaxVerion(int maxVersions) 设置表中数据的最大版本
如果只需要保存最新版本的数据 那么可以设置 setMaxVersions(1)

Time To Live
创建表的时候 可以通过 HColumnDescriptor.setTimeToLive(int timeToLive) 设置表中数据的存储生命期
过期数据将自动被删除 例如如果只需要存储最近两天的数据 那么可以设置 setTimeToLive(2 * 24 * 60 * 60)

Compact & Split
在 HBase 中 数据在更新时首先写入 WAL 日志(HLog) 和内存 (MemStore) 中 MemStore 中的数据是排序的 当 MemStore
累计到一定阀值时 就会创建一个新的 MemStore 并且将老的 MemStore 添加到 flush 队列 由单独的线程 flush 到磁盘上
成为一个 StoreFile 于此同时 系统会在 zookeeper 中记录一个 redo point 表示这个时刻之前的变更已经持久化了 
(minor compact)

StoreFile 是只读的 一旦创建后就不可以再修改 因此 Hbase 的更新其实是不断追加的操作 当一个 Store 中的 StoreFile
达到一定的阀值后 就会进行一次合并 (major compact) 将对同一个 key 的修改合并到一起 形成一个大的 StoreFile 
当 StoreFile 的大小达到一定阀值后 又会对 StoreFile 进行分割 (split) 等分为两个 StoreFile

由于对表的更新不断追加的 处理读请求时 需要访问 Store 中全部的 StoreFile 和 MemStore 将它们按照 row key 进行合并
由于 StoreFile 和 MemStore 都是经过排序的 并且 StoreFile 带有内存中索引 通常合并过程还是比较快的

实际应用中 可以考虑必要时行动进行 major compact 将同一个 row key 的修改进行合并形成一个大的 StoreFile
同时 可以将 StoreFile 设置大些 减少 split 的发生

hbase 为了防止小文件 (被刷到磁盘的 memstore) 过多 以保证保证查询效率 hbase 需要在必要的时候将这些小的 
store file 合并成相对较大的 store file 这个过程就称之为 compaction 在 hbase 中 主要存在两种类型的
compaction : minor compaction 和 major compaction

major compaction 的功能是所有 store file 合并成一个 触发 major compaction 的可能条件有
major_compact 命令 majorCompact() API, region server 自动运行 (相关参数 
hbase.hregion.majorcompaction 默认为 24小时 hbase.hregion.majorcompaction.jetter 默认值为 0.2
防止 region server 在同一时间进行 major.compaction) hbase.hregion.majorcompaction.jetter 
参数的作用是 对参数 hbase.hregion.majorcompaction 规定的值起到浮动的作用 假如两个参数都为默认值 24 和
0.2 那么 major compact 最终使用的数值为 19.2~28.8 这个范围

minor compaction 的运行机制要复杂一些 它由以下几个参数共同决定

hbase.hstore.compaction.min 默认值为 3 表示至少需要三个满足条件的 store file 时 minor compaction 才会启动

hbase.hstore.compaction.max 默认值为 10 表示一次 minor compaction 中最多选取 10 个 store file

hbase.hstore.compaction.min.size 表示文件大小小于该值的 store file 一定会加入到 minor compaction
的 store file 中

hbase.hstore.compaction.max.size 表示文件大小大于该值的 store file 一定会被 minor compaction 排除

hbase.hstore.compaction.ratio 将 store file 按照文件年龄排序 (older to younger), minor compaction
总是从 older store file 开始选择

HBase 性能优化方法 (写表操作)

多 HTable 并发写
创建多个 HTable 客户端用于写操作 提高写数据的吞吐量
static final Configuration conf = HBaseConfiguration.create();
static final String table_log_name = "user_log";
wTableLog = new HTable[tableN];
for (int i = 0; i < tableN; i ++) {
    wTableLog[i] = new HTable(conf, table_log_name);
    wTableLog[i].setWriteBufferSize(5 * 1024 * 1024);
    wTableLog[i].setAutoFlush(false);
}

HTable 参数设置

Auto Flush
通过调用 HTable.setAutoFlush(false) 方法可以将 HTable 写客户端的启动 flush 关闭 这样可以批量写入数据到 HBase
而不是有一条 put 就执行一次更新 只有当 put 填满客户端写缓存时 才实际向 HBase 服务端发起写请求 默认情况下 auto flush
是开启的

Write Buffer
通过调用 HTable.setWriteBufferSize(writeBufferSize) 方法可以设置 HTable 客户端的写 buffer 大小'
如果新设置的 buffer 小于当前写 buffer 中的数据时 buffer 将会被 flush 到服务端 其中 writeBufferSize 的单位
是 byte 字节数 可以根据实际写入数据量的多少来设置该值

WAL Flag
在 HBase 中 客户端向集群中的 RegionServer 提交数据时 (Put/Delete 操作) 首先会先写 WAL (Write Ahead Log)
日志(即 HLog 一个 RegionServer 上的所有 Region 共享一个 HLog) 只有当 WAL 日志写成功后 再接着写 MemStore
然后客户端被通知提交数据成功 如果写 WAL 日志失败 客户端则被通知提交失败 这样做的好处是可以做到 RegionServer 
宕机后的数据恢复

因此 对于相对不太重要的数据 可以在 Put/Delete 操作时 通过调用 Put.setWriteToWAL(false) 或 
Delete.setWriteToWAL(false) 函数 放弃写 WAL 日志 从而提高数据写入的性能

值行注意的是 谨慎选择关闭 WAL 日志 因为这样的话 一旦 RegionServer 宕机 Put/Delete 的数据将会无法
根据 WAL 日志进行恢复

批量写
能过调用 HTable.put(Put) 方法可以将一个指定的 row key 记录写入 HBase 同样 HBase 提供了另一个方法
通过调用 HTable.put(List<Put>) 方法可以将指定的 row key 列表 批量写入多行记录 这样做的好处是批量执行
只需要一次网络 I/O 开销 这对于数据实时性要求高 网络传输 RTT 高的情景下可能带来明显的性能提升

多线程并发写
在客户端开启多个 HTable 写线程 每个写线程负责一个 HTable 对象的 flush 操作 这样结合定时
flush 和写 buffer (writeBufferSize) 可以既保证在数据量小的时候 数据可以在较短时间内被
flush (如 1 秒内) 同时又保证在数据量大的时候 写 buffer 一满就及时进行 flush  如

for (int i = 0; i < threadN; i ++) {
    Thread th = new Thread() {
        public void run() {
            while (true) {
                try {
                    sleep(1000);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                synchronized (wTableLog[i]) {
                    try {
                        wTableLog[i].flushCommits();
                    } catch (IOException e) {
                        e.printStackTrace();
                    }
                }
            }
        }
    };
    
    th.setDaemon(true);
    th.start();
}

HBase 性能优休方法三 (读表操作)
多 HTable 并发读 创建多个 HTable 客户端用于读操作 提高读数据的吞吐量 如
static final Configuration conf = HBaseConfiguration.create();
static final String table_log_name = "user_log";
rTableLog = new HTable[tableN];
for (int i = 0; i < tableN; i ++) {
    rTableLog[i] = new HTable(conf, table_log_name);
    rTableLog[i].setScannerCaching(50);
}

HTable 参数设置
Scanner Caching
hbase.client.scanner.caching 配置项可以设置 Hbase scanner 一次从服务端抓取的数据条数 默认情况下一次一条
通过将其设置成一个合理的值 可以减少 scan 过程中 next() 的时间开销 代价是 scanner 需要通过客户端的内存来维持这些被
cache 的行记录

有三个地方可以进行配置 三者的优先级越来越高
1) 在 HBase 的 conf 配置文件中进行配置
2) 通过调用 HTable.setScannerCaching(int scannerCaching) 进行配置
3) 通过调用 Scan.setCaching(int caching) 进行配置 

Scan Attribute Selection
scan 时指定需要的 Column Family 可以减少网络传输数据量 否则默认 scan 操作会返回整行 所有 Column Family 的数据

Close ResultScanner
通过 scan 取完数据后 记得要关闭 ResultScanner 否则 RegionServer 可能会出现问题 (对应的 Server 资源无法释放)

批量读
通过调用 HTable.get(Get) 方法可以根据一个指定的 row key 获取一行记录 同样 HBase 提供了另一个方法
通过调用 HTable.get(List<Get>) 方法可以根据一个指定的 row key 列表 批量获取多行记录 这样做的好处是批量执行
只需要一次网络 I/O 开销 这对于数据实时性要求高而且网络传输 RTT 高的情景下可能带来明显的性能提升

多线程并发读
在客户端开启多个 HTable 读线程 每个读线程负责通过 HTable 对象进行 get 操作 下面是一个多线程并发读取 HBase
获取店铺一天内各分钟 PV 值的例子

public class DataReaderServer {
    // 获取店铺一天内各分钟 PV 值的入口函数
    public static ConcurrentHashMap<String, String>
    getUnitMinutePV(long uid, long startStamp, long endStamp) {
        long min = startStamp;
        int count = (int)((endStamp - startStamp) / (60 * 1000));
        List<String> lst = new ArrayList<String>();
        for (int i = 0; i <= count; i ++) {
            min = startStamp + i * 60 * 1000;
            lst.add(uid + "_" + min);
        }
        return parallelBatchMinutePV(1st);
    }
    // 多线程并发查询 获取分钟 PV 值
    private static ConcurrentHashMap<String, String>
    parallelBatchMinutePV(List<String> 1stKeys) {
        ConcurrentHashMap<String, String> hashRet = new ConcurrentHashMap<String, String>();
        int parallel = 3;
        List<List<String>> lstBatchKeys = null;
        if (lstKeys.size() < parallel) {
            lstBatchKeys = new ArrayList<List<String>>(1);
            lstBatchKeys.add(lstKeys);
        } else {
           lstBatchKeys = new ArrayList<List<String>>(parallel);
           for (int i = 0; i < parallel; i ++) {
               List<String> lst = new ArrayList<String>();
               lstBatchKeys.add(lst);
           }
           
           for (int i = 0; i <lstKeys.size(); i ++) {
               lstBatchKeys.get(i % parallel).add(lstKeys.get(i));
           }
        }
        
        List<Future<ConcurrentHashMap<String, String>>> futures = new 
        ArrayList<Future<ConcurrentHashMap<String, String>>>(5);
        
        ThreadFactoryBuilder builder = new ThreadFactoryBuilder();
        builder.setNameFormat("ParallelBatchQuery");
        ThreadFactory factory = builder.build();
        ThreadPoolExecutor executor = (ThreadPoolExecutor) 
        Executors.newFixedThreadPool(1stBatchKeys.size(), factory);
        
        for (List<String> keys : 1stBatchKeys) {
            Callable<ConcurrentHashMap<String, String>> callable = new BatchMinutePVCallable(keys);
            FutureTask<ConcurrentHashMap<String, String>> future = 
            (FutureTask<ConcurrentHashMap<String, String>>) executor.submit(callable);
            futures.add(future);
        }
        executor.shutdown();
        
        try {
            boolean stillruning = !executor.awaitTermination(5000000,
            TimeUnit.MILLISECONDS);
            if (stillRunning) {
                try {
                    executor.shutdownNow();
                } catch (Exception e) {
                    e.printStackTrace();
                }
            }
        } catch (InterruptedException e) {
            try {
                Thread.currentThread().interrupt();
            } catch (Exception e1) {
                e1.printStackTrace();
            }
        }
        
        for (Future f : futures) {
            try {
                if (f.get() != null) {
                    hashRet.putAll((ConcurrentHashMap<String, String>)f.get());
                }
            } catch (InterruptedException e) {
                try {
                    Thread.currentThread().interrupt();
                } catch (Exception el) {
                    el.printStackTrace();
                }
            } catch (ExecutionException e) {
                e.printStackTrace();
            }
        }
        return hashRet;
    }
    // 一个线程批量查询 获取分钟 PV 值
    protected static ConcurrentHashMap<String, String> getBatchMinutePV(List<String> lstKeys) {
        ConcurrentHashMap<String, String> hashRet = null;
        List<Get> lstGet = new ArrayList<Get>();
        String[] splitValue = null;
        for (String s : lstKeys) {
            splitValue = s.split("_");
            long uid = Long.parseLong(splitValue[0]);
            long min = Long.parseLong(splitValue[1]);
            byte[] key = new byte[16];
            Bytes.putLong(key, 0, uid);
            Bytes.putLong(key, 8, min);
            Get g = new Get(key);
            g.addFamily(fp);
            lstGet.add(g);
        }
        Result[] res = null;
        try {
            res = tableMinutePV[rand.nextInt(tableN)].get(lstGet);
        } catch (IOException el) {
            logger.error("tableMinutePV exception, e = " + el.getStackTrace());
        }
        
        if (res != null && res.length > 0) {
            hashRet = new ConcurrentHashMap<String, String>(res.length);
            for (Result re : res) {
                if (re != null && !re.isEmpty()) {
                    try {
                        byte[] key = re.getRow();
                        byte[] value = re.getValue(fp, cp);
                        if (key != null && value != null) {
                            hashRet.put (String.valueOf(Bytes.toLong(key, Bytes.SIZEOF_LONG)),
                            String.valueOf(Bytes.toLong(value)));
                        }
                    } catch (Exception e2) {
                        logger.error(e2.getStackTrace());
                    }
                }
            }
        }
        return hashRet;
    }
}

// 调用接口类 实现 Callable 接口
class BatchMinutePVCallable implements Callable<ConcurrentHashMap<String, String>> {
    private List<String> keys;
    
    public BatchMinutePVCallable(List<String> lstKeys) {
        this.keys = lstKeys;
    }
    
    public ConcurrentHashMap<String, String> call() throws Exception {
        return DataReadServer.getBatchMinutePV(keys);
    }
}

缓存查询结果
对于频繁查询 HBase 的应用场景 可以考虑在应用程序中做缓存 当有新的查询请求时 首先在缓存中查找 
如果存在则直接返回 不再查询 HBase 否则对 HBase 发起读请求查询 然后在应用程序中将查询结果缓存起来
至于缓存的替换策略 可以考虑 LRU 等常用的策略

Blockcache
HBase 上 Regionserver 的内存分类两个部分 一部分作为 Memstore 主要用来写 另外一部分作为 BlockCache
主要用于读

写请求会先写入 Memstore Regionserver 会给每个 region 提供一个 Memstore 当 Memstore 满 64 MB 以后
会启动 flush 刷新到磁盘 当 Memstore 的总大小超过限制时 
(heapsize * hbase.regionserver.global.memstore.upperLimit * 0.9) 会强行启动 flush 进程
从最大的 Memstore 开始 flush 直到低于限制

读请求先到 Memstore 中查数据 查不到就到 BlockCache 中查 再查不到就会到磁盘上读 并把读的结果放入 BlockCache
由于 BlockCache 采用的是 LRU 策略 因此 BlockCache 达到上限 (heapsize * hfile.block.cache.size * 0.85)
后 会启动淘汰制 淘汰掉最老的一批数据

一个 Regionserver 上有一个 BlockCache 和 N 个 Memstore 它们的大小之和不能大于等于 heapsize * 0.8
否则 HBase 不能启动 默认 BlockCache 为 0.2 而 Memstore 为 0.4 对于注重读响应时间的系统 可以将 BlockCache
设大些 比如设置 BlockCache = 0.4, Memstore = 0.39 以加大缓存的命中率

HTable 和 HTablePool 使用注意事项
HTable HTablePool 都是 HBase 客户端 API 的一部分 可以使用它们对 HBase 表进行 CRUD 操作

HTable 
HTable 是 HBase 客户端与 HBase 服务端通讯的 Java API 对象 客户端可能通过 HTable 对象与服务端进行
CRUD 操作 (增删改查) 它的创建很简单

Configuration conf = HBaseConfiguration.create();
HTable table = new HTable(conf, "tablename");

HTablePool pool = new HTablePool(conf, 10);
HTableInterface table = pool.getTable("t_student");
table.put("");
table.get("");
table.getScanner("");

HTable 使用时的一些注意事项
规避 HTable 对象的创建开销
因为客户端创建 HTable 对象后  需要运行一系列的操作 检查 .META 表确认指定名称的 HBase 表是否存在
表是否有效等等 整个时间开销比较重 可能会耗时几钞钟之长 因此最好在程序启动时一次性创建完成需要的 HTable
对象 如果使用 Java API 一般来说是在构造函数中进行创建 程序启动后直接重用

HTable 对象不是线程安全的
HTable 对象对于客户端读写数据来说不是线程安全的 因此多线程时 要为每个线程单独创建复用一个 HTable
对象 不同对象间不要共享 HTable 对象使用 特别是在客户端 auto flash 被置为 false 时 由于存在本地
write buffer 可能导致数据不一致

HTable 对象之间共享 Configuration
HTable 对象共享 Configuration 对象 这样的好处在于
共享 Zookeeper 的连接 每个客户端需要与 Zookeeper 建立连接 查询用户的 table regions 位置
这些信息可以在连接建立后缓存起来共享使用
共享公共的资源 客户端需要通过 Zookeeper 查找 ROOT 和 META 表 这个需要网络传输开销 客户端缓存这些
公共资源后能够减少后续的网络传输开销 加快查找过程速度

HTable table1 = new HTable("table1");
HTable table2 = new HTable("table2");

下面的方式更有效些
Configuration conf = HBaseConfiguration.create();
HTable table1 = new HTable(conf, "table1");
HTable table2 = new HTable(conf, "table2");

备注 即使是高负载的多线程程序 也并没有发现因为共享 Configuration 而导致的性能问题
如果你的实际情况中不是如此 那么可以尝试不共享 Configuration

HTablePool
HTablePool 可以解决 HTable 存在的线程不安全问题 同时通过维护固定数量的 HTable 对象
能够在程序运行期间复用这些 HTable 资源对象
Configuration conf = HBaseConfiguration.create();
HTablePool pool = new HTablePool(conf, 10);

HTablePool 可以自动创建 HTable 对象 而且对客户端来说使用上是完全透明的  可以避免多线程间数据并发修改问题
HTablePool 中的 HTable 对象之间是公用 Configuration 连接的 能够可以减少网络开销

HTablePool 的使用很简单 每次进行操作前 通过 HTablePool 的 getTable 方法取得一个 HTable 对象
然后进行了 put/get/scan/delete 等操作 最后通过 HTablePool 的 putTable 方法将 HTable 对象放回到 HTablePool 中

public void createUser(String username, String firstName, String lastName, String email, 
String password, String roles) throws IOException {
    HTable table = rm.getTable(UserTable,NAME);
    Put put = new Put(Bytes.toBytes(username));
    put.add(UserTable.DATA_FAMILY, UserTable.FIRSTNAME, Bytes.toBytes(firstName));
    put.add(UserTable.DATA_FAMILY, UserTable.LASTNAME, Bytes.toBytes(lastName));
    put.add(UserTable.DATA_FAMILY, UserTable.EMAIL, Bytes.toBytes(email));
    put.add(UserTable.DATA_FAMILY, UserTable.CREDENTIALS, Bytes.toBytes(password));
    put.add(UserTable.DATA_FAMILY, UserTable.ROLES, Bytes.toBytes(roles));
    table.put(put);
    table.flushCommits();
    rm.putTable(table);
}