Apache Spark is a fast and general engine for large scale data processing

引擎
快速的 DAG Memory
通用的 一栈式的大数据处理解决方案
数据处理/数据分析
大数据集

MapReduce 与 Spark
快
Run programs up to 100x faster than 
编程简单
Write applications quickly in Java Scala or Python
运行和数据处理
runs everywhere

09 年开始
10 年开源
13 年加入 apache 发展开始特别快
现在峰会越来越多

MapReduce                                      Spark
数据存储结构 磁盘 hdfs 文件系统的 split        使用内存构建弹性分布式数据集 RDD, 对数据进行运算和 cache
编程范式 Map + Reduce                          DAG(有向无环图) Transformation + action
计算中间数据落磁盘 io及序列化 反序列代代价大   计算中间数据在内存中维护 存取速度是磁盘的多个数量级
Task 以进程的方式维护 任务启动就有数秒         Task 以纯种的方式维护 对小数据集的读取能达到亚秒级的延迟

MapReduce 与 Spark 相比 有哪些异同点
基本原理上
MapReduce 基于磁盘的大数据批量处理系统
Spark 基于 RDD (弹性分布式数据集) 数据处理 显式的将 RDD 数据存储到磁盘和内存中
模型上
MapReduce 可以处理超大规模的数据 适合日志分析挖掘等 较少的迭代的长任务需求 结合了数据的分布式的计算
Spark 适合数据的挖掘 机器学习等多轮迭代式计算任务
容错性上
数据容错性 节点容错性
Spark Linage 

spark 大数据处理技术

spark 需要技术点
Java SE 基础 Scala 语言
Hadoop 2.x 有所认识 有一定的基础 基本东西
英语的要求

spark 下载
http://spark.apache.org/downloads.html

下载页需要选择spark 版本 还要选择 hadoop 版本
http://www.apache.org/dyn/closer.lua/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz

spark 的编译
sbt 编译
maven 编译
打包编译 make-distribution.sh

sudo mkdir data
sudo chown -R hadoop:hadoop data
cp spark-1.3.0-src.zip /opt/data
cd /opt/data
unzip spark-

maven 编译
jdk
maven
echo $JAVA_HOME
java -version
echo $MAVEN_HOME
mvn -version
cd spark
http://spark.apache.org/docs/1.6.0/building-spark.html

解压
tar -zxvf spark-1.

mvn 编译
mvn clean package
-DskipTests -Phadoop-2.4 \
-Dhadoop.version=2.6.0 -Pyarn \
-Phive-0.13.1 -Phive-thriftserver

make-distribution 编译
./make-distribution.sh --tgz -Pyarn -Phadoop-2.4 -Dhadoop.version=2.6.0-cdh5.4.0  -Phive-0.13.1 -Phive-thriftserver

http://archive.cloudera.com/cdh5/cdh/5/
sudo vi /etc/resolv.conf
nameserver 8.8.8.8
nameserver 8.8.4.4

cd .m2
vi settings.xml
<mirror>
    <id>nexux-osc</id>
    <mirrorOf>*</mirrorOf>
    <name>Nexus osc</name>
    <url>http://maven.oschina.net/content/groups/public/</url>
</mirror>

# 注释 129行 ... 然后自己手写
VERSION=1.3.0 # 配置 spark 的版本
SPARK_HADOOP_VERSION=2.6.0-cdh5.4.0 # 配置 hadoop 版本
SPARK_HIVE=1 # 1 表示需要将 hive 的打包进去 非 1 数字表示不打包 hive

[root@peng1 ~]# mkdir spark
[root@peng1 ~]# mv spark-1.6.0.tgz spark
[root@peng1 ~]# cd spark/
[root@peng1 spark]# ls
spark-1.6.0.tgz
[root@peng1 spark]# tar -zxvf spark-1.6.0.tgz

[root@peng1 spark]# ln -sf /root/spark/spark-1.6.0 /home/spark
[root@peng1 spark]# cd /home/spark/conf/

[root@peng1 conf]# cp -a spark-env.sh.template spark-env.sh
[root@peng1 conf]# vi spark-env.sh

export JAVA_HOME=/usr/local/java/jdk1.8.0_73
export SCALA_HOME=/home/scala
export SPARK_MASTER_IP=peng1
export SPARK_WORKER_MEMORY=1g
export SPARK_EXECUTOR_MEMORY=1g
export SPARK_DRIVER_MEMORY=1G
export HADOOP_CONF_DIR=/home/hadoop/etc/hadoop

[root@peng1 conf]# cp -a slaves.template slaves
[root@peng1 conf]# vi slaves
peng2
peng3
peng4

[root@peng1 conf]# cp -a spark-defaults.conf.template spark-defaults.conf
[root@peng1 conf]# vi spark-defaults.conf
spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://peng1:9000/historyserverforSpark
spark.yarn.historyServer.address peng1:18080
spark.history.fs.logDirectory    hdfs://peng1:9000/historyserverforSpark


[root@peng1 spark]# vi /etc/profile
export SCALA_HOME=/home/scala
export PATH=$PATH:$SCALA_HOME/bin

export SPARK_HOME=/home/spark
export PATH=$PATH:$SPARK_HOME/bin
export PATH=$PATH:$SPARK_HOME/sbin


[root@peng1 spark]# scp /etc/profile root@peng2:/etc/
profile                                       100% 2282     2.2KB/s   00:00
[root@peng1 spark]# scp /etc/profile root@peng3:/etc/
profile                                       100% 2282     2.2KB/s   00:00
[root@peng1 spark]# scp /etc/profile root@peng4:/etc/
profile                                       100% 2282     2.2KB/s   00:00

[root@peng1 spark]# scp -r /root/spark/spark-1.6.0 root@peng2:/root/spark/
[root@peng1 spark]# scp -r /root/spark/spark-1.6.0 root@peng3:/root/spark/
[root@peng1 spark]# scp -r /root/spark/spark-1.6.0 root@peng4:/root/spark/

[root@peng2 spark]# ln -sf /root/spark/spark-1.6.0/ /home/spark

[root@peng1 spark]# ./sbin/start-all.sh
http://peng1:8080/

[root@peng1 spark]# ./sbin/start-history-server.sh
http://peng1:18080
