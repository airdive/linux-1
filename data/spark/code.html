[root@peng1 ~]# cd /home/spark/
[root@peng1 spark]# ./bin/spark-shell
scala> val rdd = sc.textFile("hdfs://peng1:9000/usr/input/wc/wc-in")
rdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at <console>:27
scala> val flat = rdd.flatMap(_.split(" "))
flat: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:29
scala> val word = flat.map((_, 1))
word: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:31
scala> val redecer = word.reduceByKey(_ + _)
redecer: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:33
scala> redecer.collect
res2: Array[(String, Int)] = Array((put,4), (hadoop,2), (get,3), (hello,3), (welcom,1), (welcome,3), (world,2))


scala> val rdd = sc.textFile("/usr/input/wc/wc-in")
rdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at textFile at <console>:27

scala> rdd.first
res3: String = welcome hadoop welcome hello
scala> rdd.count
res4: Long = 6
